# # Response

## Properties

Name | Type | Description | Notes
------------ | ------------- | ------------- | -------------
**metadata** | **array<string,string>** | Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.   Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters. |
**temperature** | **float** | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or &#x60;top_p&#x60; but not both. | [default to 1]
**top_p** | **float** | An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. | [default to 1]
**user** | **string** | A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). | [optional]
**previous_response_id** | **string** | The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about  [conversation state](/docs/guides/conversation-state). | [optional]
**model** | [**\JeedomOpenAI\Model\ModelIdsResponses**](ModelIdsResponses.md) |  |
**reasoning** | [**\JeedomOpenAI\Model\Reasoning**](Reasoning.md) |  | [optional]
**max_output_tokens** | **int** | An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). | [optional]
**instructions** | **string** | Inserts a system (or developer) message as the first item in the model&#39;s context.  When using along with &#x60;previous_response_id&#x60;, the instructions from a previous response will be not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses. |
**text** | [**\JeedomOpenAI\Model\ResponsePropertiesText**](ResponsePropertiesText.md) |  | [optional]
**tools** | [**\JeedomOpenAI\Model\Tool[]**](Tool.md) | An array of tools the model may call while generating a response. You  can specify which tool to use by setting the &#x60;tool_choice&#x60; parameter.  The two categories of tools you can provide the model are:  - **Built-in tools**: Tools that are provided by OpenAI that extend the   model&#39;s capabilities, like [web search](/docs/guides/tools-web-search)   or [file search](/docs/guides/tools-file-search). Learn more about   [built-in tools](/docs/guides/tools). - **Function calls (custom tools)**: Functions that are defined by you,   enabling the model to call your own code. Learn more about   [function calling](/docs/guides/function-calling). |
**tool_choice** | [**\JeedomOpenAI\Model\ResponsePropertiesToolChoice**](ResponsePropertiesToolChoice.md) |  |
**truncation** | **string** | The truncation strategy to use for the model response. - &#x60;auto&#x60;: If the context of this response and previous ones exceeds   the model&#39;s context window size, the model will truncate the    response to fit the context window by dropping input items in the   middle of the conversation.  - &#x60;disabled&#x60; (default): If a model response will exceed the context window    size for a model, the request will fail with a 400 error. | [optional] [default to 'disabled']
**id** | **string** | Unique identifier for this Response. |
**object** | **string** | The object type of this resource - always set to &#x60;response&#x60;. |
**status** | **string** | The status of the response generation. One of &#x60;completed&#x60;, &#x60;failed&#x60;,  &#x60;in_progress&#x60;, or &#x60;incomplete&#x60;. | [optional]
**created_at** | **float** | Unix timestamp (in seconds) of when this Response was created. |
**error** | [**\JeedomOpenAI\Model\ResponseError**](ResponseError.md) |  |
**incomplete_details** | [**\JeedomOpenAI\Model\ResponseAllOfIncompleteDetails**](ResponseAllOfIncompleteDetails.md) |  |
**output** | [**\JeedomOpenAI\Model\OutputItem[]**](OutputItem.md) | An array of content items generated by the model.  - The length and order of items in the &#x60;output&#x60; array is dependent   on the model&#39;s response. - Rather than accessing the first item in the &#x60;output&#x60; array and    assuming it&#39;s an &#x60;assistant&#x60; message with the content generated by   the model, you might consider using the &#x60;output_text&#x60; property where   supported in SDKs. |
**output_text** | **string** | SDK-only convenience property that contains the aggregated text output  from all &#x60;output_text&#x60; items in the &#x60;output&#x60; array, if any are present.  Supported in the Python and JavaScript SDKs. | [optional]
**usage** | [**\JeedomOpenAI\Model\ResponseUsage**](ResponseUsage.md) |  | [optional]
**parallel_tool_calls** | **bool** | Whether to allow the model to run tool calls in parallel. | [default to true]

[[Back to Model list]](../../README.md#models) [[Back to API list]](../../README.md#endpoints) [[Back to README]](../../README.md)
